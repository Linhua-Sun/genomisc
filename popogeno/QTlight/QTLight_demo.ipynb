{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fetch relevant files from stacks populations run__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ln -s /media/chrishah/STORAGE/RAD/stacks/ALL/mapping/excl_PCR_dupl/BWA-8MM/M_zebra/3-populations/Diplotaxodon_4pop/m5_mpop5_kernel_iterate_ONE_SNP_PER_TAG/r_0.8-p_4/batch_1.vcf.gz .\n",
    "ln -s /media/chrishah/STORAGE/RAD/stacks/ALL/mapping/excl_PCR_dupl/BWA-8MM/M_zebra/3-populations/Diplotaxodon_4pop/populationmap .\n",
    "mkdir matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create 10 Bayenv input files with 5000 randomly selected loci in each__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#pip install pyvcf\n",
    "\n",
    "for a in {1..10}\n",
    "do\n",
    "    echo -e \"\\nrepitition $a:\\n\"\n",
    "    python /home/chrishah/Dropbox/Github/genomisc/popogeno/vcf_2_bayenv.py batch_1.vcf.gz --min_number 6 -r 5000 -o matrix/random_5000_rep_$a -m populationmap\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create 10 covariance matrizes with 100000 iterations each__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd matrix/\n",
    "\n",
    "for a in {1..10}\n",
    "do\n",
    "    /home/chrishah/src/Bayenv/bayenv2 0 -p 4 -r -$RANDOM -k 100000 -i random_5000_rep_$a.bayenv.SNPfile > random_5000_rep_$a.log\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__construct average covariance matrix from 10 random sets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "main_list = []\n",
    "\n",
    "for a in range(10):\n",
    "    current = \"matrix/random_5000_rep_\"+str(a+1)+\"_it-1e5.matrix\"\n",
    "#    print current\n",
    "    IN = open(current,\"r\")\n",
    "    temp_list = []\n",
    "    for line in IN:\n",
    "        temp_list.extend(line.rstrip().split(\"\\t\"))\n",
    "\n",
    "    for i in range(len(temp_list)):\n",
    "        if a == 0:\n",
    "            main_list.append([float(temp_list[i])])\n",
    "        else:\n",
    "            main_list[i].append(float(temp_list[i]))\n",
    "        \n",
    "#print main_list\n",
    "\n",
    "av_out_list = []\n",
    "std_out_list = []\n",
    "for j in range(len(main_list)):\n",
    "    av_out_list.append(np.mean(main_list[j]))\n",
    "\n",
    "#print av_out_list\n",
    "\n",
    "outstring = \"\"\n",
    "for z in range(len(av_out_list)):\n",
    "    av_out_list[z] = \"%s\\t\" %av_out_list[z]\n",
    "    if not outstring:\n",
    "        outstring = av_out_list[z]\n",
    "    else:\n",
    "        outstring = outstring+av_out_list[z]\n",
    "        if ((z+1) % 4 == 0):\n",
    "            outstring = \"%s\\n\" %(outstring)\n",
    "\n",
    "\n",
    "OUT = open(\"matrix/av_matrix.matrix\",\"w\")\n",
    "OUT.write(outstring)\n",
    "OUT.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prepare__ environmental data - average and normalize\n",
    "\n",
    "raw data is provided in a csv file with the first column containing the population id. See example in test-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize (csv, norm_prefix=\"\", normalize=True, boxplot=False, boxplots_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    The function parses a csv file and outputs normalized values\n",
    "    and boxplots if desired\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    import numpy as np\n",
    "    import subprocess\n",
    "    \n",
    "    if normalize and not norm_prefix:\n",
    "        raise IOError(\"You have to specify a prefix for the output files containing the normalized data - use 'norm_prefix='\")\n",
    "        \n",
    "    if boxplot and not boxplots_prefix:\n",
    "        raise IOError(\"You have to specify a prefix for the boxplot files to be produced - use 'boxplots_prefix='\")\n",
    "            \n",
    "    populations = []    \n",
    "    columns = {}\n",
    "    indices = defaultdict(list)\n",
    "    normalized = defaultdict(list)\n",
    "    \n",
    "    IDs = []\n",
    "    pops = []\n",
    "    \n",
    "    INFILE = open(csv, 'r')\n",
    "\n",
    "    headers = INFILE.readline().strip().split(\",\")\n",
    "#    print headers\n",
    "    headers.pop(0)\n",
    "    IDs = sorted(headers)\n",
    "    for env in headers:\n",
    "        columns[env] = []\n",
    "        \n",
    "    for line in INFILE:\n",
    "        line = line.strip()\n",
    "        temp=line.split(\",\")\n",
    "        populations.append(temp.pop(0))\n",
    "#        print population\n",
    "        for i in range(len(temp)):\n",
    "#            print \"%i:%s\\n\" %(i, header[i])\n",
    "            columns[headers[i]].append(float(temp[i]))\n",
    "#        print columns\n",
    " \n",
    "    pops = list(sorted(set(populations)))\n",
    "#    print pops\n",
    "    #find indexes for each\n",
    "    for pop in pops:\n",
    "#        print \"finding indices for %s\" %pop\n",
    "        for i in range(len(populations)):\n",
    "            if pop == populations[i]:\n",
    "#                print i\n",
    "                indices[pop].append(i)\n",
    "\n",
    "#    print indices\n",
    "    \n",
    "#    print \"\\nCalculating means\\n\"\n",
    "    for env in headers:\n",
    "        per_pop = {}\n",
    "        for pop in pops:\n",
    "#            print pop\n",
    "#            print \"%s - should be %i\" %(env, len(indices[pop]))       \n",
    "            per_pop_list = []\n",
    "            for i in indices[pop]:\n",
    "                per_pop_list.append(columns[env][i])\n",
    "                \n",
    "#            print per_pop_list\n",
    "#            print \"%s mean: %s\" %(pop, np.mean(per_pop_list))\n",
    "#            print \"%s mean: %s\" %(env, np.mean(columns[env]))\n",
    "#            print \"%s sd: %s\" %(env, np.std(columns[env]))\n",
    "            per_pop[pop] = per_pop_list\n",
    "            norm = (np.mean(per_pop_list) - np.mean(columns[env])) / np.std(columns[env])\n",
    "#            print norm\n",
    "            normalized[env].append(norm)\n",
    "        if boxplot:\n",
    "            print \"Creating boxplot for %s\\n\" %env\n",
    "            Rscript = boxplots_prefix+env+'.R'\n",
    "            FH = open(Rscript, 'w')\n",
    "            for pop in pops:\n",
    "#                print per_pop[pop]\n",
    "                vector = \"\"\n",
    "                for v in per_pop[pop]:\n",
    "                    vector+=repr(v)+','\n",
    "                FH.write(pop+' <- c(%s)\\n' %vector[:-1])\n",
    "            FH.write(\"svg(filename = '\"+boxplots_prefix+env+\".svg')\\n\") #svg(filename = '$prefix-top-$cutoff.svg')\n",
    "            FH.write(\"boxplot(%s, names = c('%s'), main = '%s')\\n\" %(\", \".join(pops), \"', '\".join(pops), env)) \n",
    "            FH.write(\"dev.off()\\n\")\n",
    "            FH.close()\n",
    "            c = subprocess.Popen(['Rscript', Rscript], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            (output,err) = c.communicate()\n",
    "\n",
    "            if err:\n",
    "                print err\n",
    "#            else:\n",
    "#                print output\n",
    "            \n",
    "#    print normalized\n",
    "    if normalize:\n",
    "        print \"\\nnormalizing %s environmental factors across %s populations\\nwriting to:\\n\\t%s.bayenv\\n\\t%s.csv\" %(len(IDs), len(pops), norm_prefix, norm_prefix)\n",
    "        OUTCSV = open(norm_prefix+'.csv', 'w')\n",
    "        OUTCSV.write(\",%s\\n\" %\",\".join(pops))\n",
    "        OUTBAYENV = open(norm_prefix+'.bayenv',\"w\")\n",
    "        for env in sorted(columns.keys()):\n",
    "            outstring = \"\"\n",
    "#            IDs.append(env)\n",
    "            for n in normalized[env]:\n",
    "                outstring += repr(n)+\"\\t\"\n",
    "\n",
    "    #        print outstring\n",
    "            OUTBAYENV.write(outstring+\"\\n\")\n",
    "            OUTCSV.write(\"%s,%s\\n\" %(env, outstring.replace('\\t',',')[:-1]))\n",
    "        \n",
    "        OUTBAYENV.close()\n",
    "        OUTCSV.close()\n",
    "    \n",
    "    return pops, IDs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "populations, IDs = normalize(csv='../Diplotaxodon_Morphometric_Data_raw.csv', normalize=True, norm_prefix='Diplotaxodon_Morphometric_Data_normalized', boxplot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print populations\n",
    "print IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__convert vcf to bayenv - generate full SNP files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir SNPfiles\n",
    "python /home/chrishah/Dropbox/Github/genomisc/popogeno/vcf_2_div.py ../batch_1.vcf.gz --min_number 6 -o SNPfiles/full_set -m ../populationmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__split up SNPfiles into single files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_for_Bayenv(infile, out_prefix):\n",
    "    \"\"\"\n",
    "    This function takes a bayenv formatted multi-SNP file,\n",
    "    splits it up into separate files (one SNP per file).\n",
    "    \"\"\"\n",
    "    SNPcount = 0\n",
    "    temp = []\n",
    "    IN = open(infile, 'r')\n",
    "    for line in IN:\n",
    "        line = line.strip()\n",
    "        temp.append(line)\n",
    "        SNPcount+=1\n",
    "        if (SNPcount % 2 == 0):\n",
    "            OUT = open('%s-%07d.txt' %(out_prefix, SNPcount/2), 'w') #out_prefix+'-'+str(SNPcount/2)+'.txt', 'w')\n",
    "#            print \"%s\\t\\n\" %\"\\n\".join(temp)\n",
    "            OUT.write(\"%s\\t\\n\" %\"\\t\\n\".join(temp))\n",
    "            OUT.close()\n",
    "            temp = []\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_for_Bayenv(infile='SNPfiles/full_set.bayenv.SNPfile', out_prefix='SNPfiles/Diplo_SNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run Bayenv for 10 replications__ serially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find the number of SNP files to add to specify in loop below\n",
    "!ls -1 SNPfiles/SNP-* |wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir running_Bayenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#adjust bayenv command to your requirements\n",
    "\n",
    "iterations=1000000\n",
    "\n",
    "cd running_Bayenv/\n",
    "\n",
    "for rep in {1..10}; do ran=$RANDOM; for a in {0000001..0021968}; do /home/chrishah/src/Bayenv/bayenv2 -i ../SNPfiles/SNP-$a.txt -e ../Nyassochromis_normalized.bayenv -m ../matrix/av_matrix.matrix -k $iterations -r -$ran -p 3 -n 14 -t -X -o bayenv_out_k100000_env_rep_$rep-rand_$ran; done > log_rep_$rep; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ALTERNATIVE__\n",
    "\n",
    "__Bayenv__ can be run on a HPC cluster in parallel. I provide a script `submit_Bayenv_array_multi.sh` that I used to run 10 replicates as arrayjob on a cluster that was running a PBS scheduling system. Total runtime for 10 replicates with 1M Bayenv iterations/SNP was ~ 24h. The results from the individual runs were then concatenated with the script `concat_sorted.sh` and moved to the directory `running_Bayenv` on the local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ANALYSE RANK STATISTICS#\n",
    "please make sure you load all functions below first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculating RANK STATISTICS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir RANK_STATISTIC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the list of Bayenv results files to be processed\n",
    "import os\n",
    "\n",
    "bayenv_res_dir = './running_bayenv/'\n",
    "bayenv_files = []\n",
    "\n",
    "for fil in os.listdir(bayenv_res_dir):\n",
    "    if fil.endswith(\".bf\"):\n",
    "        print(bayenv_res_dir+\"/\"+fil)\n",
    "        bayenv_files.append(bayenv_res_dir+\"/\"+fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print bayenv_files\n",
    "print \"\\n%i\" %len(bayenv_files)\n",
    "print IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rank_results = calculate_rank_stats(SNP_map=\"SNPfiles/full_set.bayenv.SNPmap\", infiles = bayenv_files, ids = IDs, prefix = 'RANK_STATISTIC/Diplo_k_1M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CREATE POPE PLOTS and extract the SNP ids in the top 5 percent__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_rank_files = []\n",
    "\n",
    "file_dir = 'RANK_STATISTIC/'\n",
    "for id in IDs:\n",
    "#    print id\n",
    "    for file in os.listdir(file_dir):\n",
    "        if file.endswith('_'+id+'.txt'):\n",
    "#            print [id,file_dir+'/'+file]\n",
    "            full_rank_files.append([id,file_dir+'/'+file])\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print full_rank_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_pope(files_list=full_rank_files, cutoff=0.95, num_replicates=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CREATE POPE PLOTS and extract the SNP ids in the top 1 percent__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_pope(files_list=full_rank_files, cutoff=0.99, num_replicates=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__find genes__ up and downstream of correlated SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make list desired rank statistic tsv files\n",
    "\n",
    "import os\n",
    "\n",
    "file_dir = 'RANK_STATISTIC/'\n",
    "\n",
    "rank_stats_files = []\n",
    "\n",
    "for file in os.listdir(file_dir):\n",
    "    if file.endswith('.tsv'):\n",
    "            print file_dir+'/'+file\n",
    "            rank_stats_files.append(file_dir+'/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gff_per_scaffold = parse_gff(gff='../../../../Nyassochromis/3_populations/Metriaclima_zebra.BROADMZ2.gtf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genes_per_analysis = find_genes(rank_stats = rank_stats_files, gff = gff_per_scaffold, distance = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotate_genes(SNPs_to_genes=genes_per_analysis, annotations='../../../../Nyassochromis/3_populations/blast2go_table_20150630_0957.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir find_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_candidates(SNPs_to_genes=genes_per_analysis, whitelist=genes_per_analysis.keys(), out_dir='./find_genes/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__remove the most extreme Bayenv results and recalculate rank stats__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir RANK_STATISTIC_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exclude_extreme_rep(dictionary = rank_results, ids = IDs, prefix = 'RANK_STATISTIC_reduced/Diplotaxodon_reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_rank_files = []\n",
    "\n",
    "file_dir = 'RANK_STATISTIC_reduced/'\n",
    "for id in IDs:\n",
    "#    print id\n",
    "    for file in os.listdir(file_dir):\n",
    "        if '_'+id+'_ex_rep' in file and file.endswith('.txt'):\n",
    "#            print [id,file_dir+'/'+file]\n",
    "            reduced_rank_files.append([id,file_dir+'/'+file])\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print reduced_rank_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_pope(files_list=reduced_rank_files, cutoff=0.95, num_replicates=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__find genes__ up and downstream of correlated SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make list desired rank statistic tsv files\n",
    "\n",
    "import os\n",
    "\n",
    "file_dir = 'RANK_STATISTIC_reduced/'\n",
    "\n",
    "rank_stats_files = []\n",
    "\n",
    "for file in os.listdir(file_dir):\n",
    "    if file.endswith('.tsv'):\n",
    "            print file_dir+'/'+file\n",
    "            rank_stats_files.append(file_dir+'/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genes_per_analysis = find_genes(rank_stats = rank_stats_files, gff = gff_per_scaffold, distance = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotate_genes(SNPs_to_genes=genes_per_analysis, annotations='../../../../Nyassochromis/3_populations/blast2go_table_20150630_0957.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir find_genes_reduced/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_candidates(SNPs_to_genes=genes_per_analysis, whitelist=genes_per_analysis.keys(), out_dir='./find_genes_reduced/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###DEFINE SOME FUNCTIONS FOR CALCULATING RANK STATS###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define the function__ to calculate averages and standard deviations of bayesfactor ranks across replicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_rank_stats(SNP_map, infiles, ids, prefix): #these options are currently not implemented, threshold = 0.01, window = 50e3, sigma_factor = 3, bootstrap_rep = 100):\n",
    "    \"\"\"\n",
    "    The function will calculate rank statistics (averages, standard deviations, etc)\n",
    "    across a number of Bayenv replicates\n",
    "    \"\"\"\n",
    "    #import \n",
    "    from collections import defaultdict\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    #define global variables\n",
    "    global_dict = defaultdict(dict)\n",
    "    glob = defaultdict(dict) #holds all information for the SNPs\n",
    "    reps = []\n",
    "    extremes = {}\n",
    "    rep_count = 0\n",
    "    return_dict = {}\n",
    "    \n",
    "    #read in SNP_map\n",
    "    SNPmap = open(SNP_map, 'r')\n",
    "    i=0\n",
    "    for line in SNPmap:\n",
    "        global_dict[i]['chrom'] = line.strip()\n",
    "        i += 1\n",
    "    print \"Total number of SNPs (according to the SNPmap): %i\" %len(global_dict)\n",
    "    \n",
    "    #assess Bayenv files\n",
    "    print \"Number of Bayenv replicates: %i\" %len(infiles)\n",
    "    \n",
    "    #assess environmental factors\n",
    "    print \"Number of environmental factors analysed: %i\" %len(ids)\n",
    "\n",
    "    #display settings\n",
    "#    print \"Sliding window settings:\"\n",
    "#    print \"\\tWindow size: %i bp\" %window\n",
    "#    print \"\\tSigma: %s\" %sigma_factor\n",
    "\n",
    "    #start processing\n",
    "    print \"parsing bayenv files\"\n",
    "    for bayenv_file in sorted(infiles):\n",
    "        \n",
    "        print \"\\nprocessing replicate %i:\\n%s\" %(rep_count, bayenv_file)\n",
    "        reps.append(bayenv_file)\n",
    "        fh = open(bayenv_file,'r')\n",
    "        j=0 #this variable will hold the index of the current SNP\n",
    "        sorting = defaultdict(dict) #This dictionary will contain the rank sorted SNPs\n",
    "        for bf in fh:\n",
    "            for factor_index in range(len(ids)):\n",
    "#                print j\n",
    "                factor = ids[factor_index]\n",
    "#                print \"processing factor %s\" %factor\n",
    "                if not glob.has_key(factor):\n",
    "                    glob[factor] = defaultdict(dict)\n",
    "            \n",
    "                if not sorting[factor].has_key(float(bf.split('\\t')[1+factor_index])):\n",
    "                    sorting[factor][float(bf.split('\\t')[1+factor_index])]=[j]\n",
    "                else:\n",
    "                    sorting[factor][float(bf.split('\\t')[1+factor_index])].append(j)\n",
    "            j += 1\n",
    "\n",
    "        \n",
    "        fh.close()\n",
    "        \n",
    "        for factor in sorting.keys(): #do sorting and add rank information to global dictionary\n",
    "#            print factor\n",
    "            rank = 0\n",
    "            for r in sorted(sorting[factor].keys()):\n",
    "##                print \"rank: %i\" %rank\n",
    "##                print \"bf: %f\" %r\n",
    "##                print sorting[r]\n",
    "##                print len(sorting[r])\n",
    "                for SNP in sorting[factor][r]:\n",
    "##                    print SNP\n",
    "                    if not glob[factor].has_key(SNP):\n",
    "                        glob[factor][SNP]['ranks'] = [rank]\n",
    "                    else:\n",
    "                        glob[factor][SNP]['ranks'].append(rank)\n",
    "                rank += len(sorting[factor][r])\n",
    "        rep_count += 1\n",
    "            \n",
    "            \n",
    "    output_columns = ['avg_rank', 'med_rank', 'std_rank', 'var_rank', 'mad_rank', 'avg_rank_rel', 'med_rank_rel', 'var_rank_rel', 'var_rank_weight', 'var_weighted_avg_rank', 'var_weighted_rel_avg_rank']\n",
    "                \n",
    "    print \"\\nSUMMARY:\\n\"\n",
    "    for fac in sorted(glob.keys()):\n",
    "#        print \"%s: %i\" %(fac, len(glob[fac]))\n",
    "        extremes[fac] = defaultdict(int)\n",
    "        variances = []\n",
    "#        for SNPid in glob[fac].keys()[0:10]:\n",
    "#            print \"%s: %s\" %(SNPid, glob[fac][SNPid]['ranks'])\n",
    "        for SNPid in glob[fac].keys():\n",
    "#            print glob[fac][SNPid]['ranks']\n",
    "#            print absolute_deviation_from_median(data=glob[fac][SNPid]['ranks'])\n",
    "#            print find_max(absolute_deviation_from_median(data=glob[fac][SNPid]['ranks']))\n",
    "            for maxi in find_max(absolute_deviation_from_median(data=glob[fac][SNPid]['ranks'])):\n",
    "                extremes[fac][maxi] += 1\n",
    "#            print extremes\n",
    "            glob[fac][SNPid]['avg_rank'] = np.mean(glob[fac][SNPid]['ranks'])\n",
    "            glob[fac][SNPid]['med_rank'] = np.median(glob[fac][SNPid]['ranks'])\n",
    "            glob[fac][SNPid]['std_rank'] = np.std(glob[fac][SNPid]['ranks'])\n",
    "            glob[fac][SNPid]['var_rank'] = np.var(glob[fac][SNPid]['ranks'])\n",
    "            glob[fac][SNPid]['mad_rank'] = mad(data=glob[fac][SNPid]['ranks'])\n",
    "            variances.append(glob[fac][SNPid]['var_rank'])\n",
    "            glob[fac][SNPid]['avg_rank_rel'] = np.mean(glob[fac][SNPid]['ranks'])/len(glob[fac])\n",
    "            glob[fac][SNPid]['med_rank_rel'] = np.median(glob[fac][SNPid]['ranks'])/len(glob[fac])\n",
    "        #find maximum variance for the current factor\n",
    "        max_var = max(variances)\n",
    "\n",
    "        print \"Wrting stats to %s\" %(prefix+'_'+fac+'.txt')\n",
    "        OUT = open(prefix+'_'+fac+'.txt','w')\n",
    "        OUT.write(\"chrom\\tbp\\tSNPID\\t\"+\"\\t\".join(output_columns)+'\\n')\n",
    "\n",
    "        for SNPid in glob[fac].keys(): #calculate relative and weighted variances\n",
    "            glob[fac][SNPid]['var_rank_rel'] = glob[fac][SNPid]['var_rank'] / max_var\n",
    "            glob[fac][SNPid]['var_rank_weight'] = 1-glob[fac][SNPid]['var_rank_rel']\n",
    "            glob[fac][SNPid]['var_weighted_avg_rank'] = glob[fac][SNPid]['var_rank_weight'] * glob[fac][SNPid]['avg_rank']\n",
    "            glob[fac][SNPid]['var_weighted_rel_avg_rank'] = glob[fac][SNPid]['var_rank_weight'] * glob[fac][SNPid]['avg_rank_rel']\n",
    "\n",
    "            temp_list = []\n",
    "#            outstring = str(SNPid)+','\n",
    "            outstring = global_dict[SNPid]['chrom']+'\\t'\n",
    "            for column in output_columns:\n",
    "#                print column\n",
    "#                print glob[fac][SNPid][column]\n",
    "                temp_list.append(str(glob[fac][SNPid][column]))\n",
    "            outstring += \"\\t\".join(temp_list)\n",
    "            OUT.write(outstring+'\\n')\n",
    "\n",
    "        OUT.close()\n",
    "        \n",
    "\n",
    "        counts = [extremes[fac][i] for i in sorted(extremes[fac])]\n",
    "        perc = [float(counts[i])/len(glob[fac])*100 for i in range(len(counts))]\n",
    "        ex = find_max([extremes[fac][i] for i in sorted(extremes[fac])])\n",
    "        print \"factor %s\\treplicate %s gave the most extreme ranks for %.2f %% of the SNPs\" %(fac, ex[0], perc[ex[0]])\n",
    "\n",
    "    return_dict['global'] = glob\n",
    "    return_dict['extremes'] = extremes\n",
    "    return_dict['SNPids'] = global_dict\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mad(data):\n",
    "    \"\"\"\n",
    "    find the 'median absolute deviation'\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "   \n",
    "    return np.median(np.abs(data - np.median(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def absolute_deviation_from_median(data):\n",
    "    \"\"\"\n",
    "    find the absolute deviations from median for a list of values\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    mads = []\n",
    "    med = np.median(data)\n",
    "    for d in data:\n",
    "        mads.append(np.abs(d-med))\n",
    "        \n",
    "    return mads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_max(data):\n",
    "    \"\"\"\n",
    "    find the index of the maximum value in a list\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    d = defaultdict(list)\n",
    "    for i, x in enumerate(data):\n",
    "        d[x].append(i)\n",
    "\n",
    "    k = max(d.keys())\n",
    "    return d[k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pope(files_list, cutoff, num_replicates):\n",
    "    \"\"\"\n",
    "    The function calls a shell script that configures and runs an \n",
    "    R script to plot pope plots and extract the top SNPs\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    for ID in sorted(files_list):\n",
    "        print \"processing %s:\" %ID[0]\n",
    "        print \"data in file: %s\" %ID[1]\n",
    "        c = subprocess.Popen(['sh','plot_pope.sh',ID[1],str(cutoff),ID[0],str(num_replicates)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        (output,err) = c.communicate()\n",
    "\n",
    "        if err:\n",
    "            print err\n",
    "        else:\n",
    "            print output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define the function__ that excludes the most extreme replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exclude_extreme_rep(dictionary, ids, prefix, cutoff=0):\n",
    "    \"\"\"\n",
    "    Calculate rank statistics while excluding the one most extreme replicate for every factor\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    import numpy as np\n",
    "    \n",
    "    output_columns = ['avg_rank', 'med_rank', 'std_rank', 'var_rank', 'mad_rank', 'avg_rank_rel', 'med_rank_rel', 'var_rank_rel', 'var_rank_weight', 'var_weighted_avg_rank', 'var_weighted_rel_avg_rank']\n",
    "    glob = defaultdict(dict)\n",
    "    \n",
    "    for fac in sorted(ids):\n",
    "        counts = [dictionary['extremes'][fac][i] for i in sorted(dictionary['extremes'][fac])]\n",
    "        perc = [float(counts[i])/len(dictionary['global'][fac])*100 for i in range(len(counts))]\n",
    "        ex = find_max([dictionary['extremes'][fac][i] for i in sorted(dictionary['extremes'][fac])])\n",
    "        print \"\\nfactor %s\\treplicate %s gave the most extreme ranks for %.2f %% of the SNPs\" %(fac, ex[0], perc[ex[0]])\n",
    "\n",
    "        if perc[ex[0]] > cutoff*100:\n",
    "            print \"will re-calculate stats without replicate %s\" %ex[0]\n",
    "        else:\n",
    "            print \"none of the replicates exceeds the %s threshold\" %cutoff\n",
    "            continue\n",
    "        \n",
    "        glob[fac] = defaultdict(dict)\n",
    "        variances = []\n",
    "        for SNPid in dictionary['global'][fac].keys():\n",
    "#            print dictionary['global'][fac][SNPid]['ranks']\n",
    "            temp_list = dictionary['global'][fac][SNPid]['ranks'][:]\n",
    "            temp_list.pop(ex[0])\n",
    "#            print temp_list\n",
    "            glob[fac][SNPid]['avg_rank'] = np.mean(temp_list)\n",
    "            glob[fac][SNPid]['med_rank'] = np.median(temp_list)\n",
    "            glob[fac][SNPid]['std_rank'] = np.std(temp_list)\n",
    "            glob[fac][SNPid]['var_rank'] = np.var(temp_list)\n",
    "            glob[fac][SNPid]['mad_rank'] = mad(data=temp_list)\n",
    "            variances.append(glob[fac][SNPid]['var_rank'])\n",
    "            glob[fac][SNPid]['avg_rank_rel'] = np.mean(temp_list)/len(dictionary['global'][fac])\n",
    "            glob[fac][SNPid]['med_rank_rel'] = np.median(temp_list)/len(dictionary['global'][fac])\n",
    "        #find maximum variance for the current factor\n",
    "        max_var = max(variances)\n",
    "\n",
    "        print \"Wrting stats to %s\" %(prefix+'_'+fac+'_ex_rep_'+str(ex[0])+'.txt')\n",
    "        OUT = open(prefix+'_'+fac+'_ex_rep_'+str(ex[0])+'.txt','w')\n",
    "        OUT.write(\"chrom\\tbp\\tSNPID\\t\"+\"\\t\".join(output_columns)+'\\n')\n",
    "\n",
    "        for SNPid in glob[fac].keys(): #calculate relative and weighted variances\n",
    "            glob[fac][SNPid]['var_rank_rel'] = glob[fac][SNPid]['var_rank'] / max_var\n",
    "            glob[fac][SNPid]['var_rank_weight'] = 1-glob[fac][SNPid]['var_rank_rel']\n",
    "            glob[fac][SNPid]['var_weighted_avg_rank'] = glob[fac][SNPid]['var_rank_weight'] * glob[fac][SNPid]['avg_rank']\n",
    "            glob[fac][SNPid]['var_weighted_rel_avg_rank'] = glob[fac][SNPid]['var_rank_weight'] * glob[fac][SNPid]['avg_rank_rel']\n",
    "\n",
    "            temp_list = []\n",
    "#            outstring = str(SNPid)+','\n",
    "            outstring = dictionary['SNPids'][SNPid]['chrom']+'\\t'\n",
    "            for column in output_columns:\n",
    "#                print column\n",
    "#                print glob[fac][SNPid][column]\n",
    "                temp_list.append(str(glob[fac][SNPid][column]))\n",
    "            outstring += \"\\t\".join(temp_list)\n",
    "            OUT.write(outstring+'\\n')\n",
    "\n",
    "        OUT.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_gff(gff):\n",
    "    \"\"\"\n",
    "    parse gff file\n",
    "    \"\"\"\n",
    "    \n",
    "    gff_dict = {}\n",
    "    \n",
    "    gff_fh = open(gff,'r')\n",
    "    for line in [line.strip() for line in gff_fh]:\n",
    "#        print line.split('\\t')\n",
    "        if line.split('\\t')[2] == 'CDS':\n",
    "            gene = line.split('\\t')[8].split(' ')[3].replace('\"','').replace(';','') #This line needs to be adujsted to the gff format\n",
    "            if not gff_dict.has_key(line.split('\\t')[0]):\n",
    "                gff_dict[line.split('\\t')[0]] = {}\n",
    "            gff_dict[line.split('\\t')[0]][line.split('\\t')[3]] = gene\n",
    "            gff_dict[line.split('\\t')[0]][line.split('\\t')[4]] = gene\n",
    "            \n",
    "    return gff_dict    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_genes(rank_stats, gff, distance):\n",
    "    \"\"\"\n",
    "    find genes up and downstream of SNPs\n",
    "    \"\"\"    \n",
    "    from collections import defaultdict\n",
    "    candidates = defaultdict(dict)\n",
    "    return_dict = defaultdict(dict)\n",
    "    \n",
    "    for tsv in rank_stats:\n",
    "        print \"processing rank statistic file: %s\" %tsv\n",
    "        ID = tsv.split('/')[-1].replace('.tsv','')\n",
    "        candidates[ID] = defaultdict(list)\n",
    "        rank_stats_fh = open(tsv, 'r')\n",
    "        rank_stats_fh.readline()\n",
    "        for SNP in [SNP.strip() for SNP in rank_stats_fh]:\n",
    "            rank_elem = SNP.split('\\t')\n",
    "            if not candidates[ID].has_key(rank_elem[0]):\n",
    "                candidates[ID][rank_elem[0]] = []\n",
    "            candidates[ID][rank_elem[0]].append(rank_elem[1:3])\n",
    "#            print candidates[tsv][rank_elem[0]]\n",
    "            \n",
    "\n",
    "    for tsv in sorted(candidates.keys()):\n",
    "        print \"%s:\" %tsv\n",
    "        for chrom in sorted(candidates[tsv]):\n",
    "#            print chrom\n",
    "            for hot in candidates[tsv][chrom]:\n",
    "                gene_list = []\n",
    "                temp = []\n",
    "                nr_genes = []\n",
    "                lower = int(hot[0])-(distance*1000)\n",
    "                upper = int(hot[0])+(distance*1000)\n",
    "#                print \"looking at %s\" %hot[0]\n",
    "                if not gff.has_key(chrom):\n",
    "#                    print \"no genes found on %s\\n\" %chrom\n",
    "                    continue                \n",
    "                else:\n",
    "                    for pos in gff[chrom].keys():\n",
    "                        temp.append(int(pos))\n",
    "                    \n",
    "                for pos in sorted(temp):\n",
    "                    if pos >= lower and pos <= upper:\n",
    "#                        print pos,gff[chrom][str(pos)]\n",
    "                        gene_list.append(gff[chrom][str(pos)])\n",
    "                    elif pos > upper:\n",
    "                        break\n",
    "                 \n",
    "                nr_genes = list(set(gene_list))\n",
    "                for unique_gene in nr_genes:\n",
    "#                    print [chrom,hot[0],hot[1],unique_gene]\n",
    "                    if not return_dict[tsv].has_key('genes'):\n",
    "                        return_dict[tsv]['columns'] = ['chrom','bp','ID','gene']\n",
    "                        return_dict[tsv]['genes'] = []\n",
    "                    return_dict[tsv]['genes'].append([chrom,hot[0],hot[1],unique_gene])\n",
    "\n",
    "        if not return_dict.has_key(tsv):\n",
    "            return_dict[tsv]['genes'] = []\n",
    "            return_dict[tsv]['columns'] = ['chrom','bp','ID','gene']\n",
    "        print \"identified %i gene(s)\" %len(return_dict[tsv]['genes'])\n",
    "        \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annotate_genes(SNPs_to_genes, annotations, whitelist=[]):\n",
    "    \"\"\"\n",
    "    fetch annotation for genes from file produced by Blast2GO\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    annotation = defaultdict(list)\n",
    "    \n",
    "    if whitelist:\n",
    "        for id in whitelist:\n",
    "            if not SNPs_to_genes.has_key(id):\n",
    "                raise IOError(\"You provide an analysis id %s that is not in the dictionary\" %id)        \n",
    "    else:\n",
    "        whitelist = SNPs_to_genes.keys()[:]\n",
    "\n",
    "    anno_fh = open(annotations, 'r')\n",
    "    header = anno_fh.readline().strip().split('\\t')\n",
    "    annotation['header'] = header[1:]\n",
    "    annotation['genes'] = defaultdict(list)\n",
    "    for line in [line.strip() for line in anno_fh]:\n",
    "        annotation['genes'][line.split('\\t')[0]] = line.split('\\t')[1:]\n",
    "        \n",
    "#    for gene in annotation['genes'].keys()[:10]:\n",
    "#        print gene,annotation['genes'][gene]\n",
    "\n",
    "    for analysis_id in whitelist:\n",
    "        print analysis_id\n",
    "        if len(SNPs_to_genes[analysis_id]['genes']) > 0:\n",
    "            print \"adding annoation for %s\" %analysis_id\n",
    "            for index in range(len(SNPs_to_genes[analysis_id]['genes'])):\n",
    "                if annotation['genes'].has_key(SNPs_to_genes[analysis_id]['genes'][index][-1]):\n",
    "                    \n",
    "                    if len(SNPs_to_genes[analysis_id]['columns']) == 4:\n",
    "#                        print SNPs_to_genes[analysis_id]['columns']\n",
    "#                        print \"extend the headers\"\n",
    "                        SNPs_to_genes[analysis_id]['columns'].extend(annotation['header'])\n",
    "#                        print SNPs_to_genes[analysis_id]['columns']\n",
    "#                    print annotation['genes'][SNPs_to_genes[analysis_id]['genes'][index][-1]]\n",
    "                    SNPs_to_genes[analysis_id]['genes'][index].extend(annotation['genes'][SNPs_to_genes[analysis_id]['genes'][index][-1]])\n",
    "                elif len(SNPs_to_genes[analysis_id]['genes'][index]) == 4 and not annotation['genes'].has_key(SNPs_to_genes[analysis_id]['genes'][index][-1]):\n",
    "                    print \"no annoation found for %s\" %SNPs_to_genes[analysis_id]['genes'][index][-1]\n",
    "            \n",
    "    \n",
    "        else:\n",
    "            print \"nothing to annotate - 0 candidate genes identified for %s\" %analysis_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_candidates(SNPs_to_genes, whitelist=[], rename=[], out_dir='./'):\n",
    "    \"\"\"\n",
    "    write out SNP to candidate genes text files (will be named *.genes.tsv)\n",
    "    \"\"\"\n",
    "    \n",
    "    if rename:\n",
    "        if not len(rename) == len(whitelist):\n",
    "            raise IOError(\"If you provide a list with new names it needs to be the same length as the whitelist\")\n",
    "\n",
    "    if whitelist:\n",
    "        for id in whitelist:\n",
    "            if not SNPs_to_genes.has_key(id):\n",
    "                raise IOError(\"You provide an analysis id %s that is not in the dictionary\" %id)        \n",
    "    else:\n",
    "        whitelist = SNPs_to_genes.keys()[:]\n",
    "                \n",
    "    for id in sorted(whitelist):\n",
    "        print id\n",
    "        if not len(SNPs_to_genes[id]['genes']) >= 1:\n",
    "            print \"0 candidate genes found\"\n",
    "            continue\n",
    "        else:\n",
    "            if len(SNPs_to_genes[id]['columns']) == 4:\n",
    "                print \"writing to: %s\" %(out_dir+id+'.genes.tsv')\n",
    "                out_fh = open(out_dir+id+'.genes.tsv','w')\n",
    "            else:\n",
    "                print \"writing to: %s\" %(out_dir+id+'.genes.annotated.tsv')\n",
    "                out_fh = open(out_dir+id+'.genes.annotated.tsv','w')\n",
    "\n",
    "            out_fh.write(\"%s\\n\" %\"\\t\".join(SNPs_to_genes[id]['columns']))\n",
    "            for gene in SNPs_to_genes[id]['genes']:\n",
    "                out_fh.write(\"%s\\n\" %\"\\t\".join(gene))\n",
    "            \n",
    "        out_fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
